{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mechanistic Interpretability Setup - Google Colab\n",
    "\n",
    "This notebook sets up a complete environment for mechanistic interpretability research.\n",
    "\n",
    "**What this does:**\n",
    "- Installs PyTorch, TransformerLens, and visualization tools\n",
    "- Downloads GPT-2 Medium or Llama 3.2 1B\n",
    "- Provides helper functions for activation extraction\n",
    "- Sets up experiment tracking with W&B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install core dependencies\n",
    "!pip install torch>=2.0.0 --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformer-lens circuitsvis transformers datasets\n",
    "!pip install plotly altair wandb einops\n",
    "!pip install ipywidgets\n",
    "\n",
    "print(\"✓ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from typing import List, Dict, Tuple, Callable, Optional\n",
    "from functools import partial\n",
    "import einops\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# CircuitsVis for interactive visualizations\n",
    "import circuitsvis as cv\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure torch\n",
    "torch.set_grad_enabled(False)  # We're doing inference by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model\n",
    "\n",
    "Choose your model:\n",
    "- `gpt2-medium` (355M params) - Good starting point\n",
    "- `gpt2-small` (117M params) - Faster for testing\n",
    "- `meta-llama/Llama-3.2-1B` (requires HF token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model\n",
    "MODEL_NAME = \"gpt2-medium\"  # or \"gpt2-small\", \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# Load model with TransformerLens\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(f\"✓ Loaded {MODEL_NAME}\")\n",
    "print(f\"  - Layers: {model.cfg.n_layers}\")\n",
    "print(f\"  - Heads: {model.cfg.n_heads}\")\n",
    "print(f\"  - d_model: {model.cfg.d_model}\")\n",
    "print(f\"  - d_head: {model.cfg.d_head}\")\n",
    "print(f\"  - d_mlp: {model.cfg.d_mlp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Functions for Activation Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(\n",
    "    model: HookedTransformer,\n",
    "    prompt: str,\n",
    "    layer: int,\n",
    "    component: str = \"resid_post\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extract activations from a specific layer and component.\n",
    "    \n",
    "    Args:\n",
    "        model: HookedTransformer model\n",
    "        prompt: Input text\n",
    "        layer: Layer index (0 to n_layers-1)\n",
    "        component: One of ['resid_pre', 'resid_mid', 'resid_post', 'attn_out', 'mlp_out']\n",
    "    \n",
    "    Returns:\n",
    "        Activation tensor of shape [batch, seq_len, d_model]\n",
    "    \"\"\"\n",
    "    cache = {}\n",
    "    \n",
    "    def hook_fn(activation, hook):\n",
    "        cache[hook.name] = activation.detach().cpu()\n",
    "    \n",
    "    hook_name = f\"blocks.{layer}.hook_{component}\"\n",
    "    model.run_with_hooks(\n",
    "        prompt,\n",
    "        fwd_hooks=[(hook_name, hook_fn)]\n",
    "    )\n",
    "    \n",
    "    return cache[hook_name]\n",
    "\n",
    "\n",
    "def get_attention_patterns(\n",
    "    model: HookedTransformer,\n",
    "    prompt: str,\n",
    "    layer: Optional[int] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Get attention patterns for all or specific layer.\n",
    "    \n",
    "    Args:\n",
    "        model: HookedTransformer model\n",
    "        prompt: Input text\n",
    "        layer: Optional layer index. If None, returns all layers.\n",
    "    \n",
    "    Returns:\n",
    "        Attention pattern tensor [batch, n_heads, seq_len, seq_len] or\n",
    "        [batch, n_layers, n_heads, seq_len, seq_len] if layer is None\n",
    "    \"\"\"\n",
    "    _, cache = model.run_with_cache(prompt)\n",
    "    \n",
    "    if layer is not None:\n",
    "        return cache[\"pattern\", layer]  # [batch, n_heads, seq_len, seq_len]\n",
    "    else:\n",
    "        # Stack all layers\n",
    "        patterns = torch.stack(\n",
    "            [cache[\"pattern\", l] for l in range(model.cfg.n_layers)],\n",
    "            dim=1\n",
    "        )\n",
    "        return patterns  # [batch, n_layers, n_heads, seq_len, seq_len]\n",
    "\n",
    "\n",
    "def visualize_attention(\n",
    "    model: HookedTransformer,\n",
    "    prompt: str,\n",
    "    layer: int,\n",
    "    head: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize attention pattern for a specific head.\n",
    "    \n",
    "    Args:\n",
    "        model: HookedTransformer model\n",
    "        prompt: Input text\n",
    "        layer: Layer index\n",
    "        head: Head index\n",
    "    \"\"\"\n",
    "    tokens = model.to_str_tokens(prompt)\n",
    "    attention = get_attention_patterns(model, prompt, layer)\n",
    "    \n",
    "    # Get specific head pattern\n",
    "    # pattern = attention[0, head].cpu().numpy()  # [seq_len, seq_len]\n",
    "    pattern = attention[0].cpu().numpy()  # [seq_len, seq_len]\n",
    "    \n",
    "    # Use CircuitsVis for interactive visualization\n",
    "    return cv.attention.attention_patterns(\n",
    "        tokens=tokens,\n",
    "        attention=pattern,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_mlp_activations(\n",
    "    model: HookedTransformer,\n",
    "    prompt: str,\n",
    "    layer: int,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Get MLP pre and post activations.\n",
    "    \n",
    "    Args:\n",
    "        model: HookedTransformer model\n",
    "        prompt: Input text\n",
    "        layer: Layer index\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (pre_activation, post_activation)\n",
    "        pre: [batch, seq_len, d_mlp]\n",
    "        post: [batch, seq_len, d_mlp]\n",
    "    \"\"\"\n",
    "    cache = {}\n",
    "    \n",
    "    def hook_fn(activation, hook):\n",
    "        cache[hook.name] = activation.detach().cpu()\n",
    "    \n",
    "    hooks = [\n",
    "        (f\"blocks.{layer}.mlp.hook_pre\", hook_fn),\n",
    "        (f\"blocks.{layer}.mlp.hook_post\", hook_fn),\n",
    "    ]\n",
    "    \n",
    "    model.run_with_hooks(prompt, fwd_hooks=hooks)\n",
    "    \n",
    "    return (\n",
    "        cache[f\"blocks.{layer}.mlp.hook_pre\"],\n",
    "        cache[f\"blocks.{layer}.mlp.hook_post\"],\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Model & Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompt\n",
    "test_prompt = \"The Eiffel Tower is located in the city of\"\n",
    "\n",
    "# Generate completion\n",
    "output = model.generate(test_prompt, max_new_tokens=5, temperature=0.0)\n",
    "print(f\"Prompt: {test_prompt}\")\n",
    "print(f\"Completion: {output}\")\n",
    "print()\n",
    "\n",
    "# Test activation extraction\n",
    "activation = get_activation(model, test_prompt, layer=5, component=\"resid_post\")\n",
    "print(f\"Activation shape: {activation.shape}\")\n",
    "\n",
    "# Test attention patterns\n",
    "attn = get_attention_patterns(model, test_prompt, layer=5)\n",
    "print(f\"Attention pattern shape: {attn.shape}\")\n",
    "print()\n",
    "\n",
    "# Visualize attention for layer 5, head 0\n",
    "print(\"Attention visualization for Layer 5, Head 0:\")\n",
    "visualize_attention(model, test_prompt, layer=5, head=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiment Tracking Setup (W&B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "def init_experiment(\n",
    "    project_name: str = \"mech-interp\",\n",
    "    experiment_name: Optional[str] = None,\n",
    "    config: Optional[dict] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Initialize experiment tracking.\n",
    "    \n",
    "    Args:\n",
    "        project_name: W&B project name\n",
    "        experiment_name: Optional custom name (auto-generated if None)\n",
    "        config: Configuration dictionary to log\n",
    "    \"\"\"\n",
    "    if experiment_name is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        experiment_name = f\"exp_{timestamp}\"\n",
    "    \n",
    "    # Initialize W&B\n",
    "    wandb.init(\n",
    "        project=project_name,\n",
    "        name=experiment_name,\n",
    "        config=config or {},\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Experiment tracking initialized: {experiment_name}\")\n",
    "\n",
    "\n",
    "# Example usage (commented out - uncomment when ready to track)\n",
    "# init_experiment(\n",
    "#     project_name=\"mech-interp\",\n",
    "#     experiment_name=\"activation_patching_demo\",\n",
    "#     config={\n",
    "#         \"model\": MODEL_NAME,\n",
    "#         \"task\": \"fact_tracing\",\n",
    "#         \"dataset\": \"counterfact\",\n",
    "#     }\n",
    "# )\n",
    "\n",
    "print(\"✓ Experiment tracking functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quick Start: Common Hook Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 1: Cache all activations\n",
    "def cache_all_activations(model: HookedTransformer, prompt: str):\n",
    "    \"\"\"Cache all activations from a forward pass.\"\"\"\n",
    "    logits, cache = model.run_with_cache(prompt)\n",
    "    return logits, cache\n",
    "\n",
    "\n",
    "# Pattern 2: Activation patching\n",
    "def patch_activation(\n",
    "    model: HookedTransformer,\n",
    "    clean_prompt: str,\n",
    "    corrupted_prompt: str,\n",
    "    layer: int,\n",
    "    component: str = \"resid_post\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Patch activation from corrupted run into clean run.\"\"\"\n",
    "    # Get corrupted activation\n",
    "    corrupted_act = get_activation(model, corrupted_prompt, layer, component)\n",
    "    \n",
    "    # Patch into clean run\n",
    "    def patch_hook(activation, hook):\n",
    "        activation[:] = corrupted_act.to(activation.device)\n",
    "        return activation\n",
    "    \n",
    "    hook_name = f\"blocks.{layer}.hook_{component}\"\n",
    "    patched_logits = model.run_with_hooks(\n",
    "        clean_prompt,\n",
    "        fwd_hooks=[(hook_name, patch_hook)]\n",
    "    )\n",
    "    \n",
    "    return patched_logits\n",
    "\n",
    "\n",
    "# Pattern 3: Mean ablation\n",
    "def mean_ablate_head(\n",
    "    model: HookedTransformer,\n",
    "    prompt: str,\n",
    "    layer: int,\n",
    "    head: int,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Mean ablate a specific attention head.\"\"\"\n",
    "    def ablate_hook(activation, hook):\n",
    "        # activation shape: [batch, seq, n_heads, d_head]\n",
    "        activation[:, :, head, :] = activation[:, :, head, :].mean(dim=1, keepdim=True)\n",
    "        return activation\n",
    "    \n",
    "    hook_name = f\"blocks.{layer}.attn.hook_result\"\n",
    "    ablated_logits = model.run_with_hooks(\n",
    "        prompt,\n",
    "        fwd_hooks=[(hook_name, ablate_hook)]\n",
    "    )\n",
    "    \n",
    "    return ablated_logits\n",
    "\n",
    "\n",
    "print(\"✓ Common hook patterns defined\")\n",
    "print(\"\\nYou're all set! Start exploring with the helper functions above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Explore Attention Patterns**: Use `visualize_attention()` on different prompts\n",
    "2. **Activation Patching**: Try the `patch_activation()` function for causal tracing\n",
    "3. **Circuit Discovery**: Mean ablate heads/neurons to find important components\n",
    "4. **SAE Training**: Train sparse autoencoders on MLP activations\n",
    "\n",
    "Useful hook names in TransformerLens:\n",
    "- `blocks.{l}.hook_resid_pre` - Residual stream before layer\n",
    "- `blocks.{l}.hook_resid_mid` - After attention, before MLP\n",
    "- `blocks.{l}.hook_resid_post` - After full layer\n",
    "- `blocks.{l}.attn.hook_pattern` - Attention patterns\n",
    "- `blocks.{l}.attn.hook_result` - Attention output per head\n",
    "- `blocks.{l}.mlp.hook_post` - MLP neuron activations (post-GELU)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
