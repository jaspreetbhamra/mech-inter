{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Analysis for Mechanistic Interpretability\n",
    "\n",
    "This notebook demonstrates how to extract and analyze activations from transformer models to understand how they represent factual knowledge.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll cover:\n",
    "1. Creating and managing fact datasets\n",
    "2. Extracting activations from multiple layers and components\n",
    "3. Computing activation statistics\n",
    "4. Visualizing activation patterns\n",
    "5. Comparing true vs false fact representations\n",
    "\n",
    "## Key Question\n",
    "**How do transformer models represent factual knowledge internally, and do true facts activate differently than false facts?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from src.utils import setup_logging, set_seed, load_model\n",
    "from src.activation_extraction import ActivationExtractor, ActivationConfig\n",
    "from src.fact_dataset import FactDataset, create_sample_dataset\n",
    "from src.visualization import (\n",
    "    plot_activation_magnitude_heatmap,\n",
    "    plot_activation_comparison,\n",
    "    plot_pca_activations,\n",
    "    plot_activation_space_comparison,\n",
    ")\n",
    "\n",
    "# Setup\n",
    "setup_logging()\n",
    "set_seed(42)\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Creating a Fact Dataset\n",
    "\n",
    "First, we'll create a dataset of factual statements. Each fact has:\n",
    "- **Subject**: The entity (e.g., \"Eiffel Tower\")\n",
    "- **Relation**: The relationship (e.g., \"located_in\")\n",
    "- **Object**: The target (e.g., \"Paris\")\n",
    "- **is_true**: Whether this is a true fact or counterfactual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset with true and false facts\n",
    "dataset = create_sample_dataset()\n",
    "\n",
    "print(f\"Dataset: {dataset}\")\n",
    "print(f\"Total facts: {len(dataset)}\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nExample facts:\")\n",
    "for i in range(6):\n",
    "    fact = dataset[i]\n",
    "    prompt = fact.to_prompt()\n",
    "    status = \"✓ TRUE\" if fact.is_true else \"✗ FALSE\"\n",
    "    print(f\"  [{status}] {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also create custom facts\n",
    "custom_dataset = FactDataset()\n",
    "\n",
    "# Add a true fact and its counterfactual\n",
    "custom_dataset.add_fact_with_counterfactual(\n",
    "    subject=\"Mount Everest\",\n",
    "    relation=\"located_in\",\n",
    "    true_object=\"Nepal\",\n",
    "    false_object=\"Switzerland\"\n",
    ")\n",
    "\n",
    "print(\"Custom facts:\")\n",
    "for prompt, is_true in custom_dataset.to_prompts(include_labels=True):\n",
    "    print(f\"  [{is_true}] {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter facts by type\n",
    "true_facts = dataset.filter(is_true=True)\n",
    "false_facts = dataset.filter(is_true=False)\n",
    "\n",
    "print(f\"True facts: {len(true_facts)}\")\n",
    "print(f\"False facts: {len(false_facts)}\")\n",
    "\n",
    "# Get prompts for model input\n",
    "true_prompts = true_facts.to_prompts()\n",
    "false_prompts = false_facts.to_prompts()\n",
    "\n",
    "print(f\"\\nFirst true prompt: {true_prompts[0]}\")\n",
    "print(f\"First false prompt: {false_prompts[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Loading a Model\n",
    "\n",
    "We'll use TransformerLens to load a pre-trained model. This library provides hooks for extracting internal activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 Small (fast for demonstration)\n",
    "# You can also use: 'gpt2-medium', 'gpt2-large', or 'meta-llama/Llama-3.2-1B'\n",
    "model = load_model('gpt2-small')\n",
    "\n",
    "print(f\"Model: {model.cfg.model_name}\")\n",
    "print(f\"  Layers: {model.cfg.n_layers}\")\n",
    "print(f\"  Heads per layer: {model.cfg.n_heads}\")\n",
    "print(f\"  Hidden dimension: {model.cfg.d_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Configuring Activation Extraction\n",
    "\n",
    "The `ActivationConfig` class lets us specify:\n",
    "- **components**: Which parts to extract (residual stream, attention output, MLP output)\n",
    "- **layers**: Which layers to analyze (default: all)\n",
    "- **aggregate_positions**: Whether to average across token positions\n",
    "- **return_cpu**: Whether to move tensors to CPU (saves GPU memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure what activations to extract\n",
    "config = ActivationConfig(\n",
    "    components=['resid_post'],  # Residual stream after each layer\n",
    "    aggregate_positions=True,    # Average across sequence positions\n",
    "    return_cpu=True,             # Move to CPU to save GPU memory\n",
    ")\n",
    "\n",
    "# Create the extractor\n",
    "extractor = ActivationExtractor(model, config)\n",
    "\n",
    "print(\"✓ Extractor configured\")\n",
    "print(f\"  Components: {config.components}\")\n",
    "print(f\"  Layers: {len(extractor.layers)} (all)\")\n",
    "print(f\"  Aggregate positions: {config.aggregate_positions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Extracting Activations\n",
    "\n",
    "Now we'll run the model on our facts and extract activations from all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract activations for true facts\n",
    "print(\"Extracting activations for true facts...\")\n",
    "true_activations = extractor.extract(true_prompts)\n",
    "\n",
    "print(\"\\nExtracting activations for false facts...\")\n",
    "false_activations = extractor.extract(false_prompts)\n",
    "\n",
    "# Get the component we're analyzing\n",
    "component = 'resid_post'\n",
    "true_acts = true_activations[component]\n",
    "false_acts = false_activations[component]\n",
    "\n",
    "print(f\"\\n✓ Extraction complete!\")\n",
    "print(f\"True activations shape: {true_acts.shape}\")  # [n_true, n_layers, d_model]\n",
    "print(f\"False activations shape: {false_acts.shape}\")  # [n_false, n_layers, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Computing Activation Statistics\n",
    "\n",
    "Let's compute summary statistics to understand the activation patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics for true facts\n",
    "true_stats = extractor.get_activation_stats({component: true_acts})\n",
    "\n",
    "# Compute statistics for false facts\n",
    "false_stats = extractor.get_activation_stats({component: false_acts})\n",
    "\n",
    "print(\"True Facts Statistics:\")\n",
    "for stat, value in true_stats[component].items():\n",
    "    print(f\"  {stat}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nFalse Facts Statistics:\")\n",
    "for stat, value in false_stats[component].items():\n",
    "    print(f\"  {stat}: {value:.4f}\")\n",
    "\n",
    "# Compare L2 norms\n",
    "diff = true_stats[component]['l2_norm'] - false_stats[component]['l2_norm']\n",
    "pct_diff = (diff / true_stats[component]['l2_norm']) * 100\n",
    "print(f\"\\nL2 Norm Difference: {diff:.4f} ({pct_diff:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Visualizing Activation Magnitudes\n",
    "\n",
    "Let's create a heatmap showing activation magnitudes across layers for all prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine true and false activations\n",
    "all_acts = torch.cat([true_acts, false_acts], dim=0)\n",
    "\n",
    "# Create labels\n",
    "labels = (\n",
    "    [f\"✓ {p[:40]}...\" for p in true_prompts] +\n",
    "    [f\"✗ {p[:40]}...\" for p in false_prompts]\n",
    ")\n",
    "\n",
    "# Plot heatmap\n",
    "fig = plot_activation_magnitude_heatmap(\n",
    "    all_acts,\n",
    "    labels=labels,\n",
    "    title=f\"Activation Magnitudes - {component}\",\n",
    "    metric='l2_norm'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: Look for patterns in which layers show stronger activations for true vs false facts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Comparing True vs False Activations\n",
    "\n",
    "Let's directly compare the activation magnitudes across layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison across all layers\n",
    "fig = plot_activation_comparison(\n",
    "    true_acts,\n",
    "    false_acts,\n",
    "    title=f\"True vs False Facts - {component}\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: This plot shows mean L2 norm ± std deviation. Do later layers show bigger differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare a specific layer in detail\n",
    "middle_layer = model.cfg.n_layers // 2\n",
    "\n",
    "fig = plot_activation_comparison(\n",
    "    true_acts,\n",
    "    false_acts,\n",
    "    layer_idx=middle_layer,\n",
    "    title=f\"Layer {middle_layer} Distribution\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Dimensionality Reduction (PCA)\n",
    "\n",
    "Use PCA to visualize the high-dimensional activation space in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze a specific layer\n",
    "layer_to_analyze = model.cfg.n_layers - 1  # Last layer\n",
    "\n",
    "# PCA visualization\n",
    "fig = plot_activation_space_comparison(\n",
    "    true_acts,\n",
    "    false_acts,\n",
    "    method='pca',\n",
    "    layer_idx=layer_to_analyze,\n",
    "    title=f\"PCA - Layer {layer_to_analyze}\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: Are true and false facts separable in activation space? What does the explained variance tell us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: t-SNE Visualization\n",
    "\n",
    "t-SNE captures non-linear structure better than PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE visualization (takes longer to compute)\n",
    "fig = plot_activation_space_comparison(\n",
    "    true_acts,\n",
    "    false_acts,\n",
    "    method='tsne',\n",
    "    layer_idx=layer_to_analyze,\n",
    "    title=f\"t-SNE - Layer {layer_to_analyze}\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: Do true and false facts cluster separately? This suggests the model has learned distinct representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Analyzing Different Components\n",
    "\n",
    "Let's compare different model components (attention output vs MLP output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract multiple components\n",
    "multi_config = ActivationConfig(\n",
    "    components=['attn_out', 'mlp_out', 'resid_post'],\n",
    "    aggregate_positions=True,\n",
    "    return_cpu=True,\n",
    ")\n",
    "\n",
    "multi_extractor = ActivationExtractor(model, multi_config)\n",
    "\n",
    "print(\"Extracting multiple components...\")\n",
    "true_multi = multi_extractor.extract(true_prompts[:5])  # Use subset for speed\n",
    "false_multi = multi_extractor.extract(false_prompts[:5])\n",
    "\n",
    "# Compare statistics across components\n",
    "print(\"\\nComponent Statistics (True Facts):\")\n",
    "for comp in multi_config.components:\n",
    "    stats = multi_extractor.get_activation_stats({comp: true_multi[comp]})\n",
    "    print(f\"  {comp}: L2 norm = {stats[comp]['l2_norm']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Layer-by-Layer Analysis\n",
    "\n",
    "Examine how representations evolve through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute L2 norms per layer\n",
    "true_norms = torch.norm(true_acts, dim=-1)  # [n_true, n_layers]\n",
    "false_norms = torch.norm(false_acts, dim=-1)  # [n_false, n_layers]\n",
    "\n",
    "# Find layers with biggest differences\n",
    "mean_diff_per_layer = (true_norms.mean(dim=0) - false_norms.mean(dim=0)).abs()\n",
    "top_layers = mean_diff_per_layer.argsort(descending=True)[:3]\n",
    "\n",
    "print(\"Top 3 layers with largest true/false differences:\")\n",
    "for i, layer in enumerate(top_layers):\n",
    "    diff = mean_diff_per_layer[layer].item()\n",
    "    print(f\"  {i+1}. Layer {layer}: difference = {diff:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: Analyzing Specific Fact Types\n",
    "\n",
    "Compare different relations (e.g., geographic facts vs people facts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by relation type\n",
    "location_facts = dataset.filter(relation='located_in', is_true=True)\n",
    "capital_facts = dataset.filter(relation='capital_of', is_true=True)\n",
    "\n",
    "if len(location_facts) > 0 and len(capital_facts) > 0:\n",
    "    location_prompts = location_facts.to_prompts()\n",
    "    capital_prompts = capital_facts.to_prompts()\n",
    "    \n",
    "    location_acts = extractor.extract(location_prompts)['resid_post']\n",
    "    capital_acts = extractor.extract(capital_prompts)['resid_post']\n",
    "    \n",
    "    # Compare\n",
    "    fig = plot_activation_comparison(\n",
    "        location_acts,\n",
    "        capital_acts,\n",
    "        title=\"Location Facts vs Capital Facts\"\n",
    "    )\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Not enough facts of each type in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "We've demonstrated:\n",
    "1. ✓ Creating and managing fact datasets\n",
    "2. ✓ Extracting activations from transformer models\n",
    "3. ✓ Computing activation statistics\n",
    "4. ✓ Visualizing activation patterns with heatmaps\n",
    "5. ✓ Comparing true vs false fact representations\n",
    "6. ✓ Using dimensionality reduction (PCA, t-SNE)\n",
    "7. ✓ Analyzing different model components\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested Experiments\n",
    "\n",
    "Here are experiments you can run to deepen your understanding:\n",
    "\n",
    "### 1. Model Comparison\n",
    "**Question**: Do larger models show clearer separation between true and false facts?\n",
    "- Compare GPT-2 Small, Medium, and Large\n",
    "- Plot PCA for each and measure cluster separation\n",
    "\n",
    "### 2. Layer Analysis\n",
    "**Question**: At which layer do representations become most discriminative?\n",
    "- Extract activations layer by layer\n",
    "- Train a linear classifier (true vs false) on each layer\n",
    "- Plot classification accuracy vs layer depth\n",
    "\n",
    "### 3. Component Importance\n",
    "**Question**: Are attention or MLP outputs more important for factual knowledge?\n",
    "- Extract `attn_out`, `mlp_out`, and `resid_post` for the same prompts\n",
    "- Compare separation in PCA space\n",
    "- Hypothesis: MLP might be more important (they're the \"memories\")\n",
    "\n",
    "### 4. Relation-Specific Patterns\n",
    "**Question**: Do different relation types activate different parts of the model?\n",
    "- Create datasets with diverse relations: `located_in`, `born_in`, `invented_by`, etc.\n",
    "- Extract activations for each relation type\n",
    "- Use PCA/t-SNE to see if relations cluster separately\n",
    "\n",
    "### 5. Prompt Engineering Effects\n",
    "**Question**: How does prompt format affect activations?\n",
    "- Try different templates: \"X is in Y\" vs \"The location of X is Y\" vs \"X, located in Y\"\n",
    "- Compare activation patterns\n",
    "- Does the model represent the same fact differently?\n",
    "\n",
    "### 6. Counterfactual Analysis\n",
    "**Question**: How do activations change as facts become \"more wrong\"?\n",
    "- Create facts with varying degrees of incorrectness:\n",
    "  - \"Paris is in France\" (TRUE)\n",
    "  - \"Paris is in Germany\" (nearby country)\n",
    "  - \"Paris is in Japan\" (far country)\n",
    "  - \"Paris is in Mars\" (impossible)\n",
    "- Plot activation distances from the true fact\n",
    "\n",
    "### 7. Fine-Tuning Impact\n",
    "**Question**: Can we make the model's representations more discriminative?\n",
    "- Fine-tune GPT-2 on factual question-answering\n",
    "- Compare before/after activation patterns\n",
    "- Does fine-tuning increase true/false separation?\n",
    "\n",
    "### 8. Activation Sparsity\n",
    "**Question**: Are true facts represented more sparsely?\n",
    "- Compute sparsity metrics (% of near-zero activations)\n",
    "- Compare true vs false facts\n",
    "- Hypothesis: True facts might use fewer, more specific neurons\n",
    "\n",
    "### 9. Temporal Dynamics\n",
    "**Question**: How do activations evolve during sequence processing?\n",
    "- Don't aggregate positions - keep full sequence\n",
    "- Plot activation trajectories through the sequence\n",
    "- When does the model \"realize\" a fact is false?\n",
    "\n",
    "### 10. Cross-Lingual Facts\n",
    "**Question**: Do multilingual models represent facts consistently across languages?\n",
    "- Use a multilingual model (mBERT, XLM-R)\n",
    "- State same fact in different languages\n",
    "- Compare activation patterns - are they similar?\n",
    "\n",
    "---\n",
    "\n",
    "## Code Template for Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template: Compare two conditions\n",
    "\n",
    "def run_experiment(condition_a_prompts, condition_b_prompts, name_a=\"A\", name_b=\"B\"):\n",
    "    \"\"\"Compare activations between two conditions.\"\"\"\n",
    "    \n",
    "    # Extract activations\n",
    "    acts_a = extractor.extract(condition_a_prompts)['resid_post']\n",
    "    acts_b = extractor.extract(condition_b_prompts)['resid_post']\n",
    "    \n",
    "    # Statistics\n",
    "    stats_a = extractor.get_activation_stats({'resid_post': acts_a})\n",
    "    stats_b = extractor.get_activation_stats({'resid_post': acts_b})\n",
    "    \n",
    "    print(f\"{name_a} L2 norm: {stats_a['resid_post']['l2_norm']:.4f}\")\n",
    "    print(f\"{name_b} L2 norm: {stats_b['resid_post']['l2_norm']:.4f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig = plot_activation_comparison(\n",
    "        acts_a, acts_b,\n",
    "        title=f\"{name_a} vs {name_b}\"\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "    # PCA\n",
    "    fig_pca = plot_activation_space_comparison(\n",
    "        acts_a, acts_b,\n",
    "        method='pca',\n",
    "        layer_idx=-1,\n",
    "        title=f\"PCA: {name_a} vs {name_b}\"\n",
    "    )\n",
    "    fig_pca.show()\n",
    "    \n",
    "    return acts_a, acts_b\n",
    "\n",
    "# Example usage:\n",
    "# run_experiment(true_prompts, false_prompts, \"True\", \"False\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
