{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Analysis: Identifying Factual Recall Heads\n",
    "\n",
    "This notebook demonstrates how to analyze attention patterns in transformer models to identify specific \"factual recall heads\" - attention heads that specialize in retrieving factual information.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll cover:\n",
    "1. Extracting attention patterns from all heads and layers\n",
    "2. Identifying subject tokens in factual statements\n",
    "3. Computing attention scores from prediction → subject\n",
    "4. Using statistical tests to find significant heads\n",
    "5. Visualizing attention patterns interactively\n",
    "6. Comparing attention for true vs false facts\n",
    "\n",
    "## Key Question\n",
    "**Which attention heads are responsible for retrieving factual knowledge, and do they behave differently for true vs false facts?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from src.utils import setup_logging, set_seed, load_model\n",
    "from src.attention_analysis import (\n",
    "    AttentionAnalyzer,\n",
    "    compute_bonferroni_correction,\n",
    "    compute_fdr_correction\n",
    ")\n",
    "from src.fact_dataset import create_sample_dataset\n",
    "from src.visualization import (\n",
    "    plot_factual_recall_heads,\n",
    "    plot_attention_to_subject,\n",
    "    plot_attention_comparison_interactive,\n",
    "    plot_head_scores_distribution,\n",
    "    plot_aggregated_attention_flow,\n",
    "    plot_top_heads_comparison,\n",
    ")\n",
    "\n",
    "# Setup\n",
    "setup_logging()\n",
    "set_seed(42)\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load Model and Dataset\n",
    "\n",
    "We'll use a smaller model (GPT-2 Small) for faster computation. The same analysis works for larger models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = load_model('gpt2-small')\n",
    "\n",
    "print(f\"Model: {model.cfg.model_name}\")\n",
    "print(f\"  Layers: {model.cfg.n_layers}\")\n",
    "print(f\"  Heads per layer: {model.cfg.n_heads}\")\n",
    "print(f\"  Total heads: {model.cfg.n_layers * model.cfg.n_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fact dataset\n",
    "dataset = create_sample_dataset()\n",
    "\n",
    "# Split into true and false\n",
    "true_dataset = dataset.filter(is_true=True)\n",
    "false_dataset = dataset.filter(is_true=False)\n",
    "\n",
    "print(f\"Dataset: {len(true_dataset)} true facts, {len(false_dataset)} false facts\")\n",
    "print(\"\\nExamples:\")\n",
    "for i in range(4):\n",
    "    fact = dataset[i]\n",
    "    print(f\"  [{i}] {'✓' if fact.is_true else '✗'} {fact.to_prompt()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Initialize Attention Analyzer\n",
    "\n",
    "The `AttentionAnalyzer` class extracts attention patterns from all heads and layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create analyzer\n",
    "analyzer = AttentionAnalyzer(model)\n",
    "\n",
    "print(\"✓ AttentionAnalyzer initialized\")\n",
    "print(f\"  Analyzing {analyzer.n_layers} layers × {analyzer.n_heads} heads\")\n",
    "print(f\"  = {analyzer.n_layers * analyzer.n_heads} total heads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Extract Attention Patterns\n",
    "\n",
    "For each prompt, we:\n",
    "1. Run the model forward pass\n",
    "2. Extract attention weights from all heads\n",
    "3. Identify subject token positions\n",
    "4. Store as an `AttentionPattern` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to extract subject tokens from facts\n",
    "def get_subject_tokens(fact):\n",
    "    \"\"\"Extract subject entity as list of words.\"\"\"\n",
    "    return fact.subject.split()\n",
    "\n",
    "# Get prompts and subjects\n",
    "true_prompts = true_dataset.to_prompts()\n",
    "false_prompts = false_dataset.to_prompts()\n",
    "\n",
    "true_subjects = [get_subject_tokens(f) for f in true_dataset.facts]\n",
    "false_subjects = [get_subject_tokens(f) for f in false_dataset.facts]\n",
    "\n",
    "print(f\"Example subject tokens: {true_subjects[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attention patterns for true facts\n",
    "print(\"Extracting attention for TRUE facts...\")\n",
    "true_patterns = analyzer.extract_attention_patterns(true_prompts)\n",
    "\n",
    "# Set subject positions for each pattern\n",
    "for i, pattern in enumerate(true_patterns):\n",
    "    pattern.subject_positions = analyzer._find_token_positions(\n",
    "        pattern.tokens, true_subjects[i]\n",
    "    )\n",
    "\n",
    "print(f\"✓ Extracted {len(true_patterns)} attention patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attention patterns for false facts\n",
    "print(\"Extracting attention for FALSE facts...\")\n",
    "false_patterns = analyzer.extract_attention_patterns(false_prompts)\n",
    "\n",
    "# Set subject positions\n",
    "for i, pattern in enumerate(false_patterns):\n",
    "    pattern.subject_positions = analyzer._find_token_positions(\n",
    "        pattern.tokens, false_subjects[i]\n",
    "    )\n",
    "\n",
    "print(f\"✓ Extracted {len(false_patterns)} attention patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Examine a Single Attention Pattern\n",
    "\n",
    "Let's look at one example in detail to understand the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine first true fact pattern\n",
    "example = true_patterns[0]\n",
    "\n",
    "print(f\"Prompt: {example.prompt}\")\n",
    "print(f\"\\nTokens: {example.tokens}\")\n",
    "print(f\"\\nSubject positions: {example.subject_positions}\")\n",
    "print(f\"Prediction position: {example.prediction_position} (last token)\")\n",
    "print(f\"\\nAttention shape: {example.attention.shape}\")  # [n_layers, n_heads, seq_len, seq_len]\n",
    "\n",
    "# Show which tokens are subjects\n",
    "print(\"\\nSubject tokens:\")\n",
    "for pos in example.subject_positions:\n",
    "    print(f\"  Position {pos}: '{example.tokens[pos]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Compute Subject Attention Scores\n",
    "\n",
    "For each head, we measure: **How much does the final token attend to the subject?**\n",
    "\n",
    "This gives us a score for each head: high score = strong attention to subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute scores for the example pattern\n",
    "example_scores = analyzer.compute_subject_attention_scores(example)\n",
    "\n",
    "print(f\"Subject attention scores shape: {example_scores.shape}\")  # [n_layers, n_heads]\n",
    "print(f\"\\nTop 5 heads by attention to subject:\")\n",
    "\n",
    "# Flatten and get top-5\n",
    "flat_scores = example_scores.flatten()\n",
    "top_5_indices = flat_scores.topk(5).indices\n",
    "\n",
    "for idx in top_5_indices:\n",
    "    layer = idx // model.cfg.n_heads\n",
    "    head = idx % model.cfg.n_heads\n",
    "    score = example_scores[layer, head].item()\n",
    "    print(f\"  Layer {layer} Head {head}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Statistical Testing - Find Factual Recall Heads\n",
    "\n",
    "Now we identify heads that attend **significantly more** to subjects for true facts than false facts.\n",
    "\n",
    "For each head:\n",
    "1. Compute mean attention for true facts\n",
    "2. Compute mean attention for false facts\n",
    "3. Run independent t-test\n",
    "4. Keep heads with p < 0.05 and effect size > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify factual recall heads\n",
    "results = analyzer.identify_factual_recall_heads(\n",
    "    true_patterns,\n",
    "    false_patterns,\n",
    "    threshold=0.05,        # p-value threshold\n",
    "    min_effect_size=0.01,  # minimum difference in attention\n",
    ")\n",
    "\n",
    "significant_heads = results['significant_heads']\n",
    "n_significant = len(significant_heads)\n",
    "total_heads = model.cfg.n_layers * model.cfg.n_heads\n",
    "\n",
    "print(f\"✓ Found {n_significant} significant heads out of {total_heads}\")\n",
    "print(f\"  ({n_significant / total_heads * 100:.1f}% of all heads)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 10 factual recall heads\n",
    "print(\"\\nTop 10 Factual Recall Heads:\")\n",
    "print(\"(Heads with strongest preference for true facts)\\n\")\n",
    "\n",
    "for i, head in enumerate(significant_heads[:10]):\n",
    "    print(f\"{i+1:2d}. Layer {head.layer:2d} Head {head.head:2d}: \"\n",
    "          f\"effect={head.score:.4f}, p={head.p_value:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Visualize All Heads with Statistical Significance\n",
    "\n",
    "This creates a 2-panel heatmap:\n",
    "- **Top**: Effect size (true - false attention)\n",
    "- **Bottom**: Statistical significance (-log10 p-value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot overview of all heads\n",
    "fig = plot_factual_recall_heads(\n",
    "    results,\n",
    "    top_k=10,  # Highlight top 10 with stars\n",
    "    title=f\"Factual Recall Heads - {model.cfg.model_name}\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**:\n",
    "- Red = True facts have higher attention (factual recall heads)\n",
    "- Blue = False facts have higher attention\n",
    "- Stars = Top significant heads\n",
    "- Brighter colors in bottom panel = more significant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Compare Top Heads\n",
    "\n",
    "Bar chart showing mean attention scores for top heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if significant_heads:\n",
    "    fig = plot_top_heads_comparison(\n",
    "        significant_heads,\n",
    "        results['true_scores'],\n",
    "        results['false_scores'],\n",
    "        top_k=10\n",
    "    )\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No significant heads found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Examine Attention Pattern of Top Head\n",
    "\n",
    "Let's visualize the attention matrix for the most important head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if significant_heads:\n",
    "    # Get top head\n",
    "    top_head = significant_heads[0]\n",
    "    layer, head = top_head.layer, top_head.head\n",
    "    \n",
    "    print(f\"Examining Layer {layer} Head {head}\")\n",
    "    print(f\"  Effect size: {top_head.score:.4f}\")\n",
    "    print(f\"  P-value: {top_head.p_value:.2e}\")\n",
    "    \n",
    "    # Visualize attention for first true fact\n",
    "    fig = plot_attention_to_subject(\n",
    "        true_patterns[0],\n",
    "        layer,\n",
    "        head,\n",
    "        title=f\"Top Head: L{layer}H{head} - True Fact\"\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "    print(\"\\nRed dashed line = subject tokens\")\n",
    "    print(\"Green dashed line = prediction token (last)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: Look at the last row (prediction token). Does it attend strongly to the subject?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Compare True vs False Attention\n",
    "\n",
    "Side-by-side comparison of the same head on true vs false facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if significant_heads and len(false_patterns) > 0:\n",
    "    fig = plot_attention_comparison_interactive(\n",
    "        true_patterns[0],\n",
    "        false_patterns[0],\n",
    "        layer,\n",
    "        head\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "    print(f\"\\nLeft: {true_patterns[0].prompt} (TRUE)\")\n",
    "    print(f\"Right: {false_patterns[0].prompt} (FALSE)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Is the attention to subject visibly different between true and false?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Distribution of Attention Scores\n",
    "\n",
    "Histogram showing the distribution of attention scores for the top head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if significant_heads:\n",
    "    # Compute scores for all patterns\n",
    "    true_scores_list = []\n",
    "    for pattern in true_patterns:\n",
    "        scores = analyzer.compute_subject_attention_scores(pattern)\n",
    "        true_scores_list.append(scores)\n",
    "    true_scores_tensor = torch.stack(true_scores_list, dim=0)\n",
    "    \n",
    "    false_scores_list = []\n",
    "    for pattern in false_patterns:\n",
    "        scores = analyzer.compute_subject_attention_scores(pattern)\n",
    "        false_scores_list.append(scores)\n",
    "    false_scores_tensor = torch.stack(false_scores_list, dim=0)\n",
    "    \n",
    "    # Plot distribution for top head\n",
    "    fig = plot_head_scores_distribution(\n",
    "        true_scores_tensor,\n",
    "        false_scores_tensor,\n",
    "        layer,\n",
    "        head\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: Overlaid histograms show whether true facts (green) consistently have higher attention than false facts (red)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: Aggregated Attention Flow\n",
    "\n",
    "Heatmap showing mean attention across all prompts and all positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average attention flow across all true facts\n",
    "fig = plot_aggregated_attention_flow(\n",
    "    true_patterns,\n",
    "    aggregation='mean',\n",
    "    title=\"Mean Attention Flow (True Facts)\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: Which layers and heads show highest average attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 13: Multiple Comparison Correction\n",
    "\n",
    "When testing many hypotheses (one per head), we need to correct for multiple comparisons.\n",
    "\n",
    "**Bonferroni correction**: Very conservative, divides α by number of tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Bonferroni correction\n",
    "sig_mask, corrected_alpha = compute_bonferroni_correction(\n",
    "    results['p_values'],\n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "n_bonferroni = sig_mask.sum().item()\n",
    "\n",
    "print(f\"Bonferroni Correction:\")\n",
    "print(f\"  Original α: 0.05\")\n",
    "print(f\"  Corrected α: {corrected_alpha:.2e}\")\n",
    "print(f\"  Significant heads: {n_bonferroni} (was {n_significant} before correction)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply FDR correction (less conservative)\n",
    "fdr_mask, adjusted_p = compute_fdr_correction(\n",
    "    results['p_values'],\n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "n_fdr = fdr_mask.sum().item()\n",
    "\n",
    "print(f\"FDR Correction (Benjamini-Hochberg):\")\n",
    "print(f\"  Significant heads: {n_fdr}\")\n",
    "print(f\"  (More lenient than Bonferroni)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 14: Analyze Specific Fact Examples\n",
    "\n",
    "Let's examine how specific facts are processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick an interesting fact pair\n",
    "fact_idx = 0\n",
    "true_fact = true_dataset[fact_idx]\n",
    "false_fact = false_dataset[fact_idx]\n",
    "\n",
    "print(f\"Analyzing fact pair {fact_idx}:\")\n",
    "print(f\"  TRUE:  {true_fact.to_prompt()}\")\n",
    "print(f\"  FALSE: {false_fact.to_prompt()}\")\n",
    "\n",
    "true_pattern = true_patterns[fact_idx]\n",
    "false_pattern = false_patterns[fact_idx]\n",
    "\n",
    "# Compute scores for each head\n",
    "true_scores = analyzer.compute_subject_attention_scores(true_pattern)\n",
    "false_scores = analyzer.compute_subject_attention_scores(false_pattern)\n",
    "\n",
    "# Find heads with biggest difference\n",
    "diff = (true_scores - false_scores).abs()\n",
    "top_diff_heads = diff.flatten().topk(5).indices\n",
    "\n",
    "print(\"\\nHeads with biggest true/false difference:\")\n",
    "for idx in top_diff_heads:\n",
    "    layer = idx // model.cfg.n_heads\n",
    "    head = idx % model.cfg.n_heads\n",
    "    true_score = true_scores[layer, head].item()\n",
    "    false_score = false_scores[layer, head].item()\n",
    "    print(f\"  L{layer}H{head}: true={true_score:.4f}, false={false_score:.4f}, \"\n",
    "          f\"diff={abs(true_score - false_score):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 15: Get Top-K Heads by Average Attention\n",
    "\n",
    "Alternative approach: find heads that attend most to subjects (regardless of true/false)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top heads by average subject attention\n",
    "top_heads = analyzer.get_top_heads(true_patterns, top_k=10)\n",
    "\n",
    "print(\"Top 10 heads by subject attention (true facts):\")\n",
    "for i, head in enumerate(top_heads):\n",
    "    print(f\"{i+1:2d}. Layer {head.layer:2d} Head {head.head:2d}: \"\n",
    "          f\"mean={head.mean_score:.4f} ± {head.std_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "We've demonstrated:\n",
    "1. ✓ Extracting attention patterns from all heads\n",
    "2. ✓ Identifying subject tokens in factual statements\n",
    "3. ✓ Computing attention from prediction → subject\n",
    "4. ✓ Statistical testing to find factual recall heads\n",
    "5. ✓ Visualizing attention patterns interactively\n",
    "6. ✓ Comparing true vs false fact attention\n",
    "7. ✓ Multiple comparison corrections (Bonferroni, FDR)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested Experiments\n",
    "\n",
    "### 1. Attention Pattern Evolution\n",
    "**Question**: Do factual recall heads emerge during training, or are they present from initialization?\n",
    "- Load checkpoints from different training steps\n",
    "- Run same analysis on each checkpoint\n",
    "- Plot: # of factual recall heads vs training step\n",
    "- Hypothesis: Factual heads emerge gradually\n",
    "\n",
    "### 2. Cross-Model Comparison\n",
    "**Question**: Are the same heads important across different model sizes?\n",
    "- Run analysis on GPT-2 Small, Medium, Large\n",
    "- Check if factual recall heads are in similar layers\n",
    "- E.g., is Layer 6 Head 3 in Small analogous to Layer 12 Head 6 in Medium?\n",
    "- Use layer/total_layers as normalized position\n",
    "\n",
    "### 3. Attention to Object Tokens\n",
    "**Question**: Do heads attend to the object (answer) differently for true vs false?\n",
    "- Modify `_find_token_positions` to find object positions\n",
    "- Compute attention: prediction → object\n",
    "- Compare with attention to subject\n",
    "- Hypothesis: False facts might have weaker subject→object attention\n",
    "\n",
    "### 4. Multi-Hop Reasoning\n",
    "**Question**: Can we trace information flow through multiple heads?\n",
    "- Create facts requiring multi-hop reasoning: \"Paris is in France. France is in Europe. Therefore, Paris is in Europe.\"\n",
    "- Use attention flow analysis to trace: subject → intermediate → final\n",
    "- Visualize the reasoning chain\n",
    "\n",
    "### 5. Intervention Experiments\n",
    "**Question**: What happens if we knock out factual recall heads?\n",
    "- Identify top 5 factual recall heads\n",
    "- Run model with these heads' attention zeroed out\n",
    "- Measure: Does accuracy on factual questions drop?\n",
    "- Control: Zero out random heads and compare\n",
    "\n",
    "### 6. Attention Entropy Analysis\n",
    "**Question**: Do factual recall heads have more focused attention?\n",
    "- Compute attention entropy for each head\n",
    "- Formula: `H = -Σ p(i) log p(i)` where p(i) is attention weight\n",
    "- Compare entropy of factual recall heads vs others\n",
    "- Hypothesis: Lower entropy = more focused = factual recall\n",
    "\n",
    "### 7. Relation-Specific Heads\n",
    "**Question**: Do different relations use different heads?\n",
    "- Group facts by relation type: location, invention, people, etc.\n",
    "- Run separate analysis for each relation\n",
    "- Find relation-specific factual recall heads\n",
    "- Visualization: Venn diagram of head sets\n",
    "\n",
    "### 8. Attention Gradient Analysis\n",
    "**Question**: How sensitive are predictions to attention weights?\n",
    "- Compute gradients: ∂loss/∂attention for each head\n",
    "- Compare gradient magnitudes for factual recall heads\n",
    "- Are factual heads more influential for correct predictions?\n",
    "\n",
    "### 9. Few-Shot Attention Patterns\n",
    "**Question**: Do in-context examples change which heads are important?\n",
    "- Prompt format: \"France's capital is Paris. Germany's capital is Berlin. Italy's capital is [MASK]\"\n",
    "- Compare attention patterns with vs without examples\n",
    "- Do different heads activate for in-context learning?\n",
    "\n",
    "### 10. Adversarial Attention\n",
    "**Question**: Can we craft inputs that fool factual recall heads?\n",
    "- Start with true fact: \"Paris is the capital of France\"\n",
    "- Add misleading context: \"Many people think Paris is in Germany, but Paris is the capital of France\"\n",
    "- Measure: Does attention shift from subject?\n",
    "- Find prompts where factual heads fail\n",
    "\n",
    "### 11. Temporal Attention Dynamics\n",
    "**Question**: How does attention evolve across the sequence?\n",
    "- Don't just look at final token\n",
    "- Compute attention to subject at each position\n",
    "- Plot: attention strength vs token position\n",
    "- When does the model \"lock on\" to the subject?\n",
    "\n",
    "### 12. Attention Head Clustering\n",
    "**Question**: Do factual recall heads have similar attention patterns?\n",
    "- Represent each head by its attention pattern vector\n",
    "- Cluster heads using k-means or hierarchical clustering\n",
    "- Are factual recall heads in the same cluster?\n",
    "- What characterizes different clusters?\n",
    "\n",
    "---\n",
    "\n",
    "## Code Template for Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template: Analyze a specific head in detail\n",
    "\n",
    "def analyze_head(layer, head, patterns, name=\"\"):\n",
    "    \"\"\"Deep dive into a specific attention head.\"\"\"\n",
    "    \n",
    "    print(f\"\\nAnalyzing Layer {layer} Head {head} ({name})\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Compute scores\n",
    "    scores = []\n",
    "    for pattern in patterns:\n",
    "        score = analyzer.compute_subject_attention_scores(pattern)[layer, head]\n",
    "        scores.append(score.item())\n",
    "    \n",
    "    # Statistics\n",
    "    mean_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "    print(f\"Mean attention to subject: {mean_score:.4f} ± {std_score:.4f}\")\n",
    "    \n",
    "    # Visualize for first pattern\n",
    "    fig = plot_attention_to_subject(\n",
    "        patterns[0],\n",
    "        layer,\n",
    "        head,\n",
    "        title=f\"L{layer}H{head} - {name}\"\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Example usage:\n",
    "# if significant_heads:\n",
    "#     top_head = significant_heads[0]\n",
    "#     analyze_head(top_head.layer, top_head.head, true_patterns, \"Top Factual Recall Head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template: Compare two sets of patterns\n",
    "\n",
    "def compare_pattern_sets(patterns_a, patterns_b, name_a=\"A\", name_b=\"B\"):\n",
    "    \"\"\"Compare attention patterns between two conditions.\"\"\"\n",
    "    \n",
    "    # Identify significant heads\n",
    "    results = analyzer.identify_factual_recall_heads(\n",
    "        patterns_a,\n",
    "        patterns_b,\n",
    "        threshold=0.05,\n",
    "        min_effect_size=0.01,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nComparing {name_a} vs {name_b}\")\n",
    "    print(f\"Significant heads: {len(results['significant_heads'])}\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig = plot_factual_recall_heads(\n",
    "        results,\n",
    "        title=f\"{name_a} vs {name_b}\"\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "# compare_pattern_sets(true_patterns, false_patterns, \"True\", \"False\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
